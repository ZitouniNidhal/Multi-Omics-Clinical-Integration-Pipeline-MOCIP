#!/usr/bin/env python3
"""
Pipeline principal multi-omiques
"""
import pandas as pd
import yaml
from pathlib import Path
import logging
from datetime import datetime

class MultiOmicsPipeline:
    """Pipeline principal pour l'int√©gration de donn√©es multi-omiques"""
    
    def __init__(self, config_path="config/config.yaml"):
        """Initialise le pipeline avec la configuration"""
        try:
            with open(config_path, 'r') as f:
                self.config = yaml.safe_load(f)
            
            print(f"‚úÖ Pipeline initialis√© : {self.config['general']['project_name']}")
            print(f"üìã Version : {self.config['general']['version']}")
            
            # Configuration du logging
            self.setup_logging()
            
        except FileNotFoundError:
            print(f"‚ùå Erreur : Fichier de configuration '{config_path}' non trouv√©")
            raise
        except yaml.YAMLError as e:
            print(f"‚ùå Erreur : Configuration YAML invalide - {e}")
            raise
    
    def setup_logging(self):
        """Configure le syst√®me de logging"""
        log_config = self.config.get('logging', {})
        log_level = log_config.get('level', 'INFO')
        log_file = log_config.get('file', 'logs/pipeline.log')
        
        # Cr√©er le r√©pertoire logs si n√©cessaire
        Path(log_file).parent.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=getattr(logging, log_level),
            format=log_config.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s'),
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger('MultiOmicsPipeline')
        self.logger.info("Syst√®me de logging configur√©")
    
    def run(self, omic_data_path, clinical_data_path, output_dir="results"):
        """Ex√©cute le pipeline complet"""
        self.logger.info("üöÄ D√©marrage du pipeline multi-omiques")
        
        try:
            # 1. Chargement des donn√©es
            self.logger.info("üìä Chargement des donn√©es")
            omic_data = self.load_data(omic_data_path)
            clinical_data = self.load_data(clinical_data_path)
            
            self.logger.info(f"Donn√©es omiques : {omic_data.shape}")
            self.logger.info(f"Donn√©es cliniques : {clinical_data.shape}")
            
            # 2. Pr√©traitement
            self.logger.info("üîß Pr√©traitement des donn√©es")
            processed_data = self.preprocess_data(omic_data, clinical_data)
            
            # 3. Int√©gration
            self.logger.info("üîó Int√©gration des donn√©es")
            integrated_data = self.integrate_data(processed_data)
            
            # 4. Export
            self.logger.info("üì§ Export des r√©sultats")
            output_paths = self.export_data(integrated_data, output_dir)
            
            self.logger.info("‚úÖ Pipeline termin√© avec succ√®s")
            
            return {
                'status': 'success',
                'output_paths': output_paths,
                'summary': self.generate_summary(integrated_data)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'ex√©cution : {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }
    
    def load_data(self, data_path):
        """Charge les donn√©es depuis un fichier CSV"""
        try:
            data = pd.read_csv(data_path)
            self.logger.info(f"‚úÖ Donn√©es charg√©es depuis {data_path} : {data.shape}")
            return data
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors du chargement de {data_path} : {str(e)}")
            raise
    
    def preprocess_data(self, omic_data, clinical_data):
        """Pr√©traite les donn√©es avec les modules de pr√©processing"""
        
        # Importer les modules de pr√©processing
        from .preprocessing.missing_values import MissingValueHandler
        from .preprocessing.normalization import OmicsNormalizer
        
        self.logger.info("üîß Pr√©traitement des donn√©es")
        
        # G√©rer les valeurs manquantes
        self.logger.info("Gestion des valeurs manquantes")
        missing_handler = MissingValueHandler(
            strategy=self.config['preprocessing']['missing_values']['strategy'],
            k=self.config['preprocessing']['missing_values'].get('k', 5)
        )
        
        omic_clean = missing_handler.fit_transform(omic_data)
        clinical_clean = missing_handler.fit_transform(clinical_data)
        
        # Normaliser les donn√©es omiques
        self.logger.info("Normalisation des donn√©es omiques")
        normalizer = OmicsNormalizer(
            method=self.config['preprocessing']['normalization']['method']
        )
        
        omic_normalized = normalizer.normalize(omic_clean)
        
        # Statistiques de pr√©processing
        preprocessing_info = {
            'omic_missing_values_before': omic_data.isnull().sum().sum(),
            'omic_missing_values_after': omic_clean.isnull().sum().sum(),
            'clinical_missing_values_before': clinical_data.isnull().sum().sum(),
            'clinical_missing_values_after': clinical_clean.isnull().sum().sum(),
            'normalization_method': self.config['preprocessing']['normalization']['method']
        }
        
        self.logger.info(f"Valeurs manquantes omiques : {preprocessing_info['omic_missing_values_before']} ‚Üí {preprocessing_info['omic_missing_values_after']}")
        self.logger.info(f"Valeurs manquantes cliniques : {preprocessing_info['clinical_missing_values_before']} ‚Üí {preprocessing_info['clinical_missing_values_after']}")
        
        return {
            'omic': omic_normalized,
            'clinical': clinical_clean,
            'preprocessing_info': preprocessing_info
        }
    
    def integrate_data(self, processed_data):
        """Int√®gre les donn√©es multi-modalit√©s avec les modules d'int√©gration"""
        
        # Importer les modules d'int√©gration
        from .integration.sample_alignment import SampleAlignment
        from .integration.data_fusion import MultiOmicsFusion
        
        self.logger.info("üîó Int√©gration des donn√©es")
        
        omic_data = processed_data['omic']
        clinical_data = processed_data['clinical']
        
        # Aligner les √©chantillons
        self.logger.info("Alignement des √©chantillons")
        aligner = SampleAlignment(
            fuzzy_matching=self.config['integration']['sample_alignment'].get('fuzzy_matching', False)
        )
        
        aligned_data = aligner.align_by_patient_id(
            {'omic': omic_data, 'clinical': clinical_data},
            {'omic': 'patient_id', 'clinical': 'patient_id'}
        )
        
        # Validation de l'alignement
        validation_report = aligner.validate_alignment(
            {'omic': omic_data, 'clinical': clinical_data}, 
            aligned_data
        )
        
        if not validation_report['alignment_successful']:
            self.logger.error("‚ùå √âchec de l'alignement des √©chantillons")
            raise ValueError("Impossible d'aligner les √©chantillons")
        
        # Fusionner les donn√©es
        self.logger.info("Fusion multi-modalit√©s")
        fusion = MultiOmicsFusion(
            fusion_method=self.config['integration']['data_fusion']['method']
        )
        
        integrated = fusion.horizontal_fusion(aligned_data, sample_key='patient_id')
        
        # Scaling optionnel des features
        if self.config['integration']['data_fusion'].get('scale_features', False):
            self.logger.info("Scaling des features")
            integrated = fusion.scale_features(integrated, method='standard')
        
        self.logger.info(f"‚úÖ Int√©gration termin√©e : {integrated.shape}")
        return integrated
    
    def export_data(self, integrated_data, output_dir):
        """Exporte les donn√©es dans diff√©rents formats avec les modules d'export"""
        
        # Importer les modules d'export
        from .standardization.json_export import JSONExporter
        from .standardization.csv_export import CSVExporter
        
        Path(output_dir).mkdir(exist_ok=True)
        
        output_paths = {}
        
        # Export CSV standardis√©
        self.logger.info("üì§ Export CSV")
        csv_exporter = CSVExporter(
            separator=self.config['export']['csv'].get('separator', '\t'),
            include_header=self.config['export']['csv'].get('include_header', True)
        )
        
        csv_path = f"{output_dir}/integrated_data.csv"
        csv_success = csv_exporter.export_standard_csv(
            integrated_data, 
            csv_path,
            {
                'pipeline_version': self.config['general']['version'],
                'export_date': datetime.now().isoformat(),
                'n_samples': len(integrated_data),
                'n_features': len(integrated_data.columns)
            }
        )
        
        if csv_success:
            output_paths['csv'] = csv_path
            self.logger.info(f"‚úÖ Donn√©es export√©es en CSV : {csv_path}")
        
        # Export JSON avec sch√©ma
        self.logger.info("üì§ Export JSON")
        json_exporter = JSONExporter(
            schema_version=self.config['export']['json']['schema_version']
        )
        
        json_path = f"{output_dir}/integrated_data.json"
        json_success = json_exporter.export_with_schema(
            integrated_data, 
            json_path,
            include_metadata=self.config['export']['json'].get('include_metadata', True)
        )
        
        if json_success:
            output_paths['json'] = json_path
            self.logger.info(f"‚úÖ Donn√©es export√©es en JSON : {json_path}")
        
        # FHIR OPTIONNEL (si temps disponible)
        if 'fhir' in self.config['export'] and self.config['export']['fhir'].get('enabled', False):
            self.logger.info("üì§ Export FHIR (optionnel)")
            # TODO: Impl√©menter FHIR export si temps disponible
            pass
        
        return output_paths
    
    def generate_summary(self, data):
        """G√©n√®re un r√©sum√© des donn√©es trait√©es"""
        return {
            'n_samples': len(data),
            'n_features': len(data.columns),
            'memory_usage_mb': data.memory_usage(deep=True).sum() / (1024*1024),
            'completeness': 1 - data.isnull().sum().sum() / (len(data) * len(data.columns))
        }

# CLI
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Pipeline Multi-Omiques")
    parser.add_argument("--config", default="config/config.yaml", help="Chemin du fichier de configuration")
    parser.add_argument("--omic-data", required=True, help="Chemin des donn√©es omiques")
    parser.add_argument("--clinical-data", required=True, help="Chemin des donn√©es cliniques")
    parser.add_argument("--output-dir", default="results", help="R√©pertoire de sortie")
    
    args = parser.parse_args()
    
    print(f"üß¨ Pipeline Multi-Omiques - Version 1.0")
    print("=" * 50)
    
    pipeline = MultiOmicsPipeline(args.config)
    result = pipeline.run(args.omic_data, args.clinical_data, args.output_dir)
    
    print("\\n" + "=" * 50)
    print("üìä R√©sultats du pipeline :")
    
    if result['status'] == 'success':
        print("‚úÖ Pipeline ex√©cut√© avec succ√®s!")
        print(f"üìÅ Fichiers de sortie : {result['output_paths']}")
        print(f"üìà R√©sum√© : {result['summary']}")
    else:
        print(f"‚ùå Erreur : {result['error']}")